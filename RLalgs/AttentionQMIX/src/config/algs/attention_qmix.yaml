# AttentionQMIX配置 - 基于dTAPE，加入注意力机制
# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 0.995
epsilon_finish: 0.05
epsilon_anneal_time: 100000

runner: "episode"
batch_size_run: 1
buffer_size: 5000

batch_size: 128

critic_mac: "cate_broadcast_comm_mac_full"
critic_agent: "rnn_agent_n"

# Comm
comm: True
comm_embed_dim: 3
comm_method: "information_bottleneck_full"
c_beta: 1.
comm_beta: 0.001
comm_entropy_beta: 1e-6
gate_loss_beta: 0.00001
only_downstream: False
use_IB: True
is_print: False

is_comm_beta_decay: False
comm_beta_start_decay: 20000000
comm_beta_target: 1e-2
comm_beta_end_decay: 50000000

is_comm_entropy_beta_decay: False
comm_entropy_beta_start_decay: 20000000
comm_entropy_beta_target: 1e-4
comm_entropy_beta_end_decay: 50000000

is_cur_mu: False
is_rank_cut_mu: False
cut_mu_thres: 1.
cut_mu_rank_thres: 80.0

# update the target network every {} episodes
target_update_interval: 200
t_max: 2005000

# use the Q_Learner to train
mac : "basic_mac_logits"
agent: "rnn"
agent_output_type: "q"
learner: "max_q_learner"
double_q: True
mixer: "attention_qmix"  # 使用注意力QMIX
mixing_embed_dim: 64  # 增加embed维度以支持注意力
hypernet_layers: 2
hypernet_embed: 128  # 增加hypernet维度

# 注意力机制参数
attention_heads: 4  # 多头注意力头数
attention_dim: 64  # 注意力维度
attention_dropout: 0.1  # 注意力dropout率

central_loss: 1
qmix_loss: 1
w: 0.5
hysteretic_qmix: True

central_mixing_embed_dim: 256
central_action_embed: 1
central_mac: "basic_central_mac"
central_agent: "central_rnn"
central_rnn_hidden_dim: 64
central_mixer: "ff"
td_lambda: 0.6
lr: 0.0008  # 略微降低学习率，注意力机制需要更稳定的训练

alpha_lr: 3e-4
alpha_init: -0.07

p: 0.5
name: "attention_qmix_env=4_adam_td_lambda"

